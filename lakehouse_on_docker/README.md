# Lakehouse Architecture - Docker Compose

Arquitectura de tipo Lakehouse completa con herramientas open source, configurada para desarrollo local en modo standalone/single node.

## üèóÔ∏è Arquitectura

Este stack incluye las siguientes herramientas:

- **Airflow**: Orquestaci√≥n de workflows y pipelines de datos
- **MinIO**: Almacenamiento S3-compatible (Data Lake)
- **Spark**: Procesamiento distribuido en modo standalone
- **Nessie**: Control de versiones para tablas (Git-like para datos)
- **Trino**: Motor de consultas SQL distribuido

## üìã Requisitos Previos

- Docker Engine 20.10+
- Docker Compose 2.0+
- Al menos 8GB de RAM disponible
- Al menos 20GB de espacio en disco

## ‚ö° Inicio R√°pido (TL;DR)

```bash
# 1. Navegar al directorio
cd lakehouse_on_docker

# 2. Crear directorios (si no existen)
mkdir -p airflow/{dags,logs,config,plugins} trino/config/catalog pyspark

# 3. Iniciar todos los servicios
docker-compose up -d
# Nota: Los permisos de Jupyter se corrigen autom√°ticamente al iniciar el contenedor

# 4. Verificar estado
docker-compose ps

# 5. Ver logs
docker-compose logs -f

# 6. Obtener token de Jupyter
docker-compose logs pyspark-jupyter | grep token
```

**Accesos r√°pidos:**
- Airflow: http://localhost:8084 (airflow/airflow)
- MinIO: http://localhost:9001 (minioadmin/minioadmin)
- Jupyter: http://localhost:8888 (token en logs)
- Trino: http://localhost:8083/ui
- Nessie: http://localhost:19120/api/v2/config

## üöÄ Inicio R√°pido

### 1. Navegar al directorio del proyecto

```bash
cd lakehouse_on_docker
```

### 2. Configurar variables de entorno (opcional)

```bash
# Copiar template de variables de entorno
cp env.template .env

# Editar .env si necesitas cambiar configuraciones (puertos, passwords, etc.)
# Por defecto, los valores funcionan sin necesidad de editar
```

### 3. Crear directorios necesarios

```bash
# Crear estructura de directorios
mkdir -p airflow/{dags,logs,config,plugins}
mkdir -p trino/config/catalog
mkdir -p pyspark
```

**Nota:** Los archivos de configuraci√≥n de Trino ya est√°n creados en `trino/config/`. Si necesitas modificarlos, edita los archivos existentes.

**‚úÖ Permisos de Jupyter - Autom√°tico:**

Los permisos de Jupyter se corrigen autom√°ticamente al iniciar el contenedor. El `docker-compose.yml` est√° configurado para ejecutar `chown` en los directorios necesarios antes de iniciar Jupyter, por lo que **no necesitas ejecutar comandos manuales**.

### 4. Iniciar todos los servicios

```bash
# Iniciar todos los servicios en segundo plano
docker-compose up -d
```

Este comando iniciar√° todos los servicios:
- `airflow-standalone` - Airflow en modo standalone
- `minio` - Almacenamiento S3-compatible
- `pyspark-jupyter` - Spark + Jupyter Notebooks
- `nessie` - Control de versiones
- `trino-coordinator` - Motor de consultas SQL

### 5. Verificar el estado de los servicios

```bash
# Ver estado de todos los contenedores
docker-compose ps

# Ver logs de todos los servicios
docker-compose logs -f

# Ver logs de un servicio espec√≠fico
docker-compose logs -f airflow-standalone
docker-compose logs -f minio
docker-compose logs -f pyspark-jupyter
docker-compose logs -f nessie
docker-compose logs -f trino-coordinator
```

### 6. Acceder a los servicios

Espera 1-2 minutos para que todos los servicios inicien completamente, luego accede a:

#### Airflow
- **URL**: http://localhost:8084
- **Usuario**: `airflow`
- **Password**: `airflow`

#### MinIO Console
- **URL**: http://localhost:9001
- **Usuario**: `minioadmin`
- **Password**: `minioadmin`

#### Jupyter Lab (Spark + PySpark)
- **URL**: http://localhost:8888
- **Token**: Obtener con el siguiente comando:
```bash
docker-compose logs pyspark-jupyter | grep -i token
# O buscar en los logs la l√≠nea que contiene "http://127.0.0.1:8888/lab?token=..."
```

#### Nessie API
- **URL**: http://localhost:19120/api/v2/config
- Verificar estado: `curl http://localhost:19120/api/v2/config`

#### Trino
- **URL**: http://localhost:8083/ui
- **Puerto SQL**: `8083` (para clientes SQL como DBeaver, etc.)

## üõ†Ô∏è Comandos √ötiles

### Iniciar servicios

```bash
# Iniciar todos los servicios
docker-compose up -d

# Iniciar un servicio espec√≠fico
docker-compose up -d airflow-standalone
docker-compose up -d minio
docker-compose up -d pyspark-jupyter
```

### Detener servicios

```bash
# Detener todos los servicios (mantiene contenedores)
docker-compose stop

# Detener y eliminar contenedores (mantiene vol√∫menes)
docker-compose down

# Detener, eliminar contenedores y vol√∫menes (‚ö†Ô∏è elimina datos)
docker-compose down -v
```

### Reiniciar servicios

```bash
# Reiniciar todos los servicios
docker-compose restart

# Reiniciar un servicio espec√≠fico
docker-compose restart airflow-standalone
```

### Ver logs

```bash
# Logs de todos los servicios en tiempo real
docker-compose logs -f

# Logs de un servicio espec√≠fico
docker-compose logs -f airflow-standalone

# √öltimas 100 l√≠neas de logs
docker-compose logs --tail=100 pyspark-jupyter
```

### Ejecutar comandos en contenedores

```bash
# Ejecutar comando en contenedor de Airflow
docker-compose exec airflow-standalone airflow version

# Acceder a shell interactivo
docker-compose exec pyspark-jupyter bash
docker-compose exec minio sh

# Ejecutar script Python en contenedor
docker-compose exec pyspark-jupyter python /home/jovyan/work/script.py

# Los notebooks se guardan en ./pyspark/ y se montan en /home/jovyan/work
```

### Obtener token de Jupyter

```bash
# M√©todo 1: Buscar en logs
docker-compose logs pyspark-jupyter | grep -i token

# M√©todo 2: Ejecutar comando en el contenedor
docker-compose exec pyspark-jupyter jupyter server list
```

## üîß Configuraci√≥n de Servicios

### Airflow

Airflow est√° configurado en **modo standalone** (un solo contenedor que ejecuta webserver, scheduler y triggerer). Usa `SequentialExecutor` con SQLite para desarrollo local. Los DAGs deben colocarse en `airflow/dags/`.

**Conexiones pre-configuradas:**
- Conexi√≥n S3 (MinIO): `aws_default`
  - Endpoint: `http://minio:9000`
  - Access Key: `minioadmin`
  - Secret Key: `minioadmin`

### MinIO

MinIO act√∫a como almacenamiento S3-compatible. Puedes crear buckets desde la consola web o usando la API.

**Ejemplo de creaci√≥n de bucket desde Python:**
```python
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

s3_hook = S3Hook(aws_conn_id='aws_default')
s3_hook.create_bucket(bucket_name='warehouse')
```

### Spark + Jupyter Notebooks

Spark est√° integrado con Jupyter Notebooks en un solo contenedor usando la imagen `jupyter/pyspark-notebook`. Esto permite desarrollar y ejecutar c√≥digo Spark directamente desde notebooks.

**Caracter√≠sticas:**
- Jupyter Lab habilitado
- PySpark pre-instalado y configurado
- Spark en modo local (standalone)
- Notebooks montados en `./pyspark`

**Acceso a Jupyter:**
1. Inicia el contenedor: `docker-compose up -d pyspark-jupyter`
2. Obt√©n el token: `docker-compose logs pyspark-jupyter | grep token`
3. Accede a: http://localhost:8888

**Ejemplo de uso en Jupyter Notebook:**
```python
from pyspark.sql import SparkSession

# Crear sesi√≥n de Spark
spark = SparkSession.builder \
    .appName("LakehouseExample") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .getOrCreate()

# Leer datos desde MinIO
df = spark.read.parquet("s3a://warehouse/data/")
df.show()
```

### Nessie

Nessie proporciona control de versiones Git-like para tus tablas. Puedes crear branches, commits, y merges.

**Ejemplo de uso con PyIceberg:**
```python
from pyiceberg.catalog import load_catalog

catalog = load_catalog(
    name="nessie",
    uri="http://nessie:19120/api/v2",
    warehouse="s3://warehouse/",
    s3_endpoint="http://minio:9000"
)
```

### Trino

Trino permite consultar datos desde m√∫ltiples fuentes usando SQL est√°ndar.

**Ejemplo de conexi√≥n:**
```sql
-- Conectar a Trino desde cualquier cliente SQL
-- Host: localhost
-- Port: 8083
-- Catalog: iceberg o nessie
-- Schema: default

SHOW CATALOGS;
USE iceberg.default;
SHOW TABLES;
```

## üìÅ Estructura de Directorios

```
lakehouse_on_docker/
‚îú‚îÄ‚îÄ docker-compose.yml      # Configuraci√≥n principal
‚îú‚îÄ‚îÄ env.template           # Template de variables de entorno
‚îú‚îÄ‚îÄ .env                    # Variables de entorno (no commiteado)
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ dags/              # DAGs de Airflow
‚îÇ   ‚îú‚îÄ‚îÄ logs/              # Logs de Airflow
‚îÇ   ‚îú‚îÄ‚îÄ config/            # Configuraci√≥n de Airflow
‚îÇ   ‚îî‚îÄ‚îÄ plugins/           # Plugins de Airflow
‚îú‚îÄ‚îÄ pyspark/               # Jupyter Notebooks con PySpark
‚îî‚îÄ‚îÄ trino/
    ‚îî‚îÄ‚îÄ config/            # Configuraci√≥n de Trino
        ‚îú‚îÄ‚îÄ config.properties
        ‚îú‚îÄ‚îÄ jvm.config
        ‚îú‚îÄ‚îÄ node.properties
        ‚îî‚îÄ‚îÄ catalog/
            ‚îú‚îÄ‚îÄ iceberg.properties
            ‚îî‚îÄ‚îÄ nessie.properties
```


## üîó Integraci√≥n entre Servicios

### Flujo de datos t√≠pico

1. **Ingesta**: Airflow orquesta la ingesta de datos a MinIO
2. **Procesamiento**: Spark procesa datos desde MinIO
3. **Versionado**: Nessie versiona las tablas procesadas
4. **Consulta**: Trino permite consultar datos versionados con SQL

### Ejemplo de pipeline completo

```python
# airflow/dags/example_pipeline.py
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator
from datetime import datetime

with DAG('lakehouse_pipeline', start_date=datetime(2024, 1, 1)) as dag:
    create_bucket = S3CreateBucketOperator(
        task_id='create_bucket',
        bucket_name='warehouse',
        aws_conn_id='aws_default'
    )
    
    spark_job = SparkSubmitOperator(
        task_id='process_data',
        application='/opt/airflow/dags/spark_job.py',
        conf={
            'spark.master': 'spark://spark-master:7077',
            'spark.hadoop.fs.s3a.endpoint': 'http://minio:9000',
        }
    )
    
    create_bucket >> spark_job
```

## ‚ö†Ô∏è Notas Importantes

1. **Puertos**: Aseg√∫rate de que los puertos configurados no est√©n en uso por otros servicios
2. **Recursos**: Este stack requiere recursos considerables. Ajusta la configuraci√≥n seg√∫n tu hardware
3. **Persistencia**: Los datos se almacenan en vol√∫menes de Docker. Usa `docker-compose down -v` con precauci√≥n
4. **Desarrollo**: Esta configuraci√≥n es para desarrollo local. No usar en producci√≥n sin ajustes de seguridad
5. **Permisos de Jupyter**: Los permisos se corrigen autom√°ticamente al iniciar el contenedor. Si encuentras problemas, simplemente reinicia el servicio con `docker-compose restart pyspark-jupyter`

## üêõ Troubleshooting

### Airflow no inicia

```bash
# Verificar logs
docker-compose logs airflow-standalone

# Verificar permisos
ls -la airflow/

# Reiniciar servicio
docker-compose restart airflow-standalone
```

### MinIO no accesible

```bash
# Verificar que el servicio est√° corriendo
docker-compose ps minio

# Verificar logs
docker-compose logs minio

# Probar conexi√≥n desde contenedor
docker-compose exec airflow-standalone curl http://minio:9000/minio/health/live
```

### Jupyter no inicia o no accesible

#### Error de permisos: `PermissionError: [Errno 13] Permission denied: '/home/jovyan/.local/share'`

**‚úÖ Soluci√≥n autom√°tica integrada:** El `docker-compose.yml` est√° configurado para corregir autom√°ticamente los permisos al iniciar el contenedor. Si a√∫n encuentras este error:

**Soluci√≥n: Reiniciar el contenedor**
```bash
# Reiniciar el servicio (los permisos se corregir√°n autom√°ticamente)
docker-compose restart pyspark-jupyter
```

Si el problema persiste despu√©s de reiniciar:

**Soluci√≥n alternativa: Eliminar y recrear el volumen**
```bash
# Detener el servicio
docker-compose stop pyspark-jupyter

# Eliminar el volumen (‚ö†Ô∏è esto eliminar√° datos guardados en .local)
docker volume rm lakehouse_on_docker_spark-jupyter-data

# Reiniciar el servicio (crear√° un nuevo volumen y corregir√° permisos autom√°ticamente)
docker-compose up -d pyspark-jupyter
```

#### Otros problemas comunes

```bash
# Verificar logs
docker-compose logs pyspark-jupyter

# Obtener token de acceso
docker-compose logs pyspark-jupyter | grep -i token

# Reiniciar servicio
docker-compose restart pyspark-jupyter

# Verificar que el contenedor est√° corriendo
docker-compose ps pyspark-jupyter

# Acceder al contenedor para debugging
docker-compose exec pyspark-jupyter bash
```

## üìö Recursos Adicionales

- [Airflow Documentation](https://airflow.apache.org/docs/)
- [MinIO Documentation](https://min.io/docs/)
- [Spark Documentation](https://spark.apache.org/docs/)
- [Nessie Documentation](https://projectnessie.org/)
- [Trino Documentation](https://trino.io/docs/)

## üìù Licencia

Este proyecto es de c√≥digo abierto y est√° disponible bajo la licencia MIT.

