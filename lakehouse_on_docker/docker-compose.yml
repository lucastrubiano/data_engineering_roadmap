version: '3.8'

# ============================================
# Lakehouse Architecture - Docker Compose
# ============================================
# Arquitectura de tipo Lakehouse con herramientas open source
# Configurado para desarrollo local (single node/standalone)
#
# Servicios incluidos:
# - Airflow: Orquestación de workflows (Standalone)
# - MinIO: Almacenamiento S3-compatible (Data Lake)
# - Spark + Jupyter: Procesamiento y análisis (Standalone)
# - Nessie: Control de versiones para tablas (Git-like para datos)
# - Trino: Motor de consultas SQL distribuido
# ============================================

x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.10.4}
  environment: &airflow-common-env
    # Modo standalone: SequentialExecutor es el adecuado para desarrollo local
    AIRFLOW__CORE__EXECUTOR: SequentialExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # Configuración para integración con MinIO (S3)
    AIRFLOW__CONNECTIONS__AWS_DEFAULT__CONN_TYPE: 's3'
    AIRFLOW__CONNECTIONS__AWS_DEFAULT__SCHEMA: 'http'
    AIRFLOW__CONNECTIONS__AWS_DEFAULT__HOST: 'minio'
    AIRFLOW__CONNECTIONS__AWS_DEFAULT__PORT: '9000'
    AIRFLOW__CONNECTIONS__AWS_DEFAULT__LOGIN: ${MINIO_ROOT_USER:-minioadmin}
    AIRFLOW__CONNECTIONS__AWS_DEFAULT__PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    AIRFLOW__CONNECTIONS__AWS_DEFAULT__EXTRA: '{"endpoint_url": "http://minio:9000"}'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/config:/opt/airflow/config
    - ./airflow/plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    minio:
      condition: service_healthy

services:
  # ==========================================
  # MinIO - Almacenamiento S3-compatible (Data Lake)
  # ==========================================
  minio:
    image: minio/minio:RELEASE.2025-07-23T15-54-02Z
    container_name: minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    ports:
      - "${MINIO_API_PORT:-9000}:9000"
      - "${MINIO_CONSOLE_PORT:-9001}:9001"
    volumes:
      - minio-data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - lakehouse-network

  # ==========================================
  # Nessie - Control de versiones para tablas
  # ==========================================
  nessie:
    image: projectnessie/nessie:latest
    container_name: nessie
    environment:
      QUARKUS_HTTP_PORT: 19120
      QUARKUS_HTTP_HOST: 0.0.0.0
      # Usar in-memory para desarrollo local (los datos se pierden al reiniciar)
      # Para producción, configurar con almacenamiento persistente
      QUARKUS_NESSIE_VERSION_STORE_TYPE: IN_MEMORY
    ports:
      - "${NESSIE_PORT:-19120}:19120"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19120/api/v2/config"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    restart: unless-stopped
    networks:
      - lakehouse-network

  # ==========================================
  # Spark + Jupyter Notebooks - Procesamiento y análisis
  # ==========================================
  # Imagen que incluye Spark y Jupyter Notebooks en un solo contenedor
  pyspark-jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: pyspark-jupyter
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_DRIVER_MEMORY=2g
      - SPARK_EXECUTOR_MEMORY=2g
    ports:
      - "${SPARK_JUPYTER_PORT:-8888}:8888"
    volumes:
      - ./pyspark:/home/jovyan/work
      - spark-jupyter-data:/home/jovyan/.local
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        chown -R 1000:1000 /home/jovyan/.local 2>/dev/null || true
        chown -R 1000:1000 /home/jovyan/work 2>/dev/null || true
        exec /usr/local/bin/start-notebook.sh
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/api"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    restart: unless-stopped
    networks:
      - lakehouse-network

  # ==========================================
  # Trino - Motor de consultas SQL distribuido
  # ==========================================
  trino-coordinator:
    image: trinodb/trino:latest
    container_name: trino-coordinator
    environment:
      - TRINO_ENV=TRINO_ENV_PLACEHOLDER
    ports:
      - "${TRINO_PORT:-8083}:8080"
    volumes:
      - ./trino/config:/etc/trino
      - trino-data:/data/trino
    command: /usr/lib/trino/bin/run-trino
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - lakehouse-network

  # ==========================================
  # Airflow - Orquestación de workflows (Standalone)
  # ==========================================
  # Airflow en modo standalone: un solo contenedor que ejecuta
  # webserver, scheduler y triggerer en un solo proceso
  airflow-standalone:
    <<: *airflow-common
    container_name: airflow-standalone
    command: standalone
    ports:
      - "${AIRFLOW_WEBSERVER_PORT:-8084}:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    depends_on:
      <<: *airflow-common-depends-on
    networks:
      - lakehouse-network

# ============================================
# VOLÚMENES
# ============================================
volumes:
  minio-data:
    driver: local
  spark-jupyter-data:
    driver: local
  trino-data:
    driver: local

# ============================================
# NETWORKS
# ============================================
networks:
  lakehouse-network:
    driver: bridge

